{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from IPython.display import HTML          \n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_metric, Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    th, td {\n",
       "        text-align: left !important;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Left align data in pd tables\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "    th, td {\n",
    "        text-align: left !important;\n",
    "    }\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data and clean \n",
    "# Data is from https://www.kaggle.com/datasets/arshkandroo/behavioural-tweets?select=Lonely_Tweets.csv.\n",
    "# Tweepy API was used to scrape twitter for lonely and normal tweets. The quality is okay but \n",
    "# some misclassification. The tweets are already cleaned using the NLTK library according to the\n",
    "# data collectors.\n",
    "\n",
    "df_normal = pd.read_csv('Data/Normal_Tweets.csv.xls')\n",
    "del df_normal['Unnamed: 0']\n",
    "df_normal.columns.values[0] = 'tweet'\n",
    "df_normal['label'] = 0                                  # 0 for normal tweets \n",
    "df_normal['id'] = range(1, len(df_normal) + 1)\n",
    "\n",
    "df_lonely = pd.read_csv('Data/Lonely_Tweets.csv.xls')\n",
    "del df_lonely['Unnamed: 0']\n",
    "df_lonely.columns.values[0] = 'tweet'\n",
    "df_lonely['label'] = 1                                  # 1 for lonely tweets\n",
    "df_lonely['id'] = range(len(df_normal) + 1, len(df_normal) + len(df_lonely) + 1)\n",
    "\n",
    "df = pd.concat([df_normal, df_lonely])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data set has 18447 tweets in it.\n",
      "\n",
      "There are 3 variables and they are as follows: \n",
      "tweet    object\n",
      "label     int64\n",
      "id        int64\n",
      "dtype: object\n",
      "\n",
      "The first and last five rows of the table are:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>remember hillary email non secure server</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cant avoid demon</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>por fin la pusieron en spotify losing way de f...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kills</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thank introduce important gunsense law make co...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8517</th>\n",
       "      <td>love son get paternity test person sex doesnt ...</td>\n",
       "      <td>1</td>\n",
       "      <td>18443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8518</th>\n",
       "      <td>tell father dont want bother need talk god cau...</td>\n",
       "      <td>1</td>\n",
       "      <td>18444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8519</th>\n",
       "      <td>lrt leave alone finally found need cry love ha...</td>\n",
       "      <td>1</td>\n",
       "      <td>18445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8520</th>\n",
       "      <td>know something dont need one love want want wa...</td>\n",
       "      <td>1</td>\n",
       "      <td>18446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8521</th>\n",
       "      <td>dont need want meet want several moment side</td>\n",
       "      <td>1</td>\n",
       "      <td>18447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  label     id\n",
       "0             remember hillary email non secure server       0      1\n",
       "1                                     cant avoid demon       0      2\n",
       "2     por fin la pusieron en spotify losing way de f...      0      3\n",
       "3                                                kills       0      4\n",
       "4     thank introduce important gunsense law make co...      0      5\n",
       "8517  love son get paternity test person sex doesnt ...      1  18443\n",
       "8518  tell father dont want bother need talk god cau...      1  18444\n",
       "8519  lrt leave alone finally found need cry love ha...      1  18445\n",
       "8520  know something dont need one love want want wa...      1  18446\n",
       "8521      dont need want meet want several moment side       1  18447"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display summary data\n",
    "print(f\"The data set has {len(df)} tweets in it.\")\n",
    "print()\n",
    "\n",
    "print(f\"There are {df.shape[1]} variables and they are as follows: \")\n",
    "print(df.dtypes)\n",
    "print()\n",
    "\n",
    "print(\"The first and last five rows of the table are:\")\n",
    "pd.concat([df.head(), df.tail()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and validation set\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_df.tweet.tolist()\n",
    "train_labels = train_df.label.tolist()\n",
    "\n",
    "labels = torch.tensor(train_labels)\n",
    "\n",
    "# Vectorize data in one step using the tokenizer\n",
    "encoded_inputs = tokenizer(\n",
    "    train_texts,\n",
    "    add_special_tokens=True,\n",
    "    truncation=True,\n",
    "    max_length=20,\n",
    "    padding='max_length',\n",
    "    return_attention_mask=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "input_ids = encoded_inputs[\"input_ids\"]\n",
    "attention_masks = encoded_inputs[\"attention_mask\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict = {\n",
    "    'input_ids': input_ids,\n",
    "    'attention_mask': attention_masks,\n",
    "    'labels': labels\n",
    "}\n",
    "\n",
    "# # Move each tensor in the dictionary to the GPU\n",
    "# train_data_dict = {key: tensor.to(\"mps\") for key, tensor in train_data_dict.items()}\n",
    "\n",
    "# # Verify each tensor's device\n",
    "# for key, tensor in train_data_dict.items():\n",
    "#     print(f\"{key} is on {tensor.device}\")\n",
    "\n",
    "# Create a Dataset object\n",
    "train_dataset = Dataset.from_dict(train_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "# print(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=0.3,\n",
    "    per_device_train_batch_size=32,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,  # The model to train\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Your training dataset\n",
    "    # Optionally add a validation dataset\n",
    "    # eval_dataset=validation_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1986214016764204b40d9930af2141cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7017, 'grad_norm': 2.767789602279663, 'learning_rate': 4.640287769784173e-05, 'epoch': 0.02}\n",
      "{'loss': 0.6608, 'grad_norm': 3.600064277648926, 'learning_rate': 4.280575539568346e-05, 'epoch': 0.04}\n",
      "{'loss': 0.5434, 'grad_norm': 6.691036701202393, 'learning_rate': 3.920863309352518e-05, 'epoch': 0.06}\n",
      "{'loss': 0.4431, 'grad_norm': 5.390189170837402, 'learning_rate': 3.561151079136691e-05, 'epoch': 0.09}\n",
      "{'loss': 0.3757, 'grad_norm': 5.2418999671936035, 'learning_rate': 3.201438848920863e-05, 'epoch': 0.11}\n",
      "{'loss': 0.3749, 'grad_norm': 3.9865145683288574, 'learning_rate': 2.841726618705036e-05, 'epoch': 0.13}\n",
      "{'loss': 0.3229, 'grad_norm': 5.72578763961792, 'learning_rate': 2.482014388489209e-05, 'epoch': 0.15}\n",
      "{'loss': 0.2577, 'grad_norm': 4.145211219787598, 'learning_rate': 2.1223021582733816e-05, 'epoch': 0.17}\n",
      "{'loss': 0.3771, 'grad_norm': 8.23632526397705, 'learning_rate': 1.7625899280575538e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3315, 'grad_norm': 4.620920181274414, 'learning_rate': 1.4028776978417266e-05, 'epoch': 0.22}\n",
      "{'loss': 0.2684, 'grad_norm': 3.9500985145568848, 'learning_rate': 1.0431654676258994e-05, 'epoch': 0.24}\n",
      "{'loss': 0.2381, 'grad_norm': 2.3106377124786377, 'learning_rate': 6.83453237410072e-06, 'epoch': 0.26}\n",
      "{'loss': 0.2663, 'grad_norm': 2.606060266494751, 'learning_rate': 3.237410071942446e-06, 'epoch': 0.28}\n",
      "{'train_runtime': 27.5089, 'train_samples_per_second': 160.934, 'train_steps_per_second': 5.053, 'train_loss': 0.391283376611394, 'epoch': 0.3}\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "# torch.mps.empty_cache()\n",
    "# model.to(\"cpu\")\n",
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"bert_model\")\n",
    "tokenizer.save_pretrained(\"bert_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
