{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding, pipeline\n",
    "from datasets import Dataset, DatasetDict\n",
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data and clean \n",
    "# Data is from https://www.kaggle.com/datasets/arshkandroo/behavioural-tweets?select=Lonely_Tweets.csv.\n",
    "# Tweepy API was used to scrape twitter for lonely and normal tweets. The quality is okay but \n",
    "# some misclassification. The tweets are already cleaned using the NLTK library according to the\n",
    "# data collectors.\n",
    "\n",
    "# The downloaded data from Kaggle was saved in a corrupted way with both csv.xls endings.\n",
    "# I used os to save them as pure csv files first. The files also contained bad first (index) \n",
    "# columns, requiring different ways of dropping these columns.\n",
    "\n",
    "df_normal = pd.read_csv('Data/Normal_Tweets.csv', index_col=0)\n",
    "df_normal.columns.values[0] = 'tweet'\n",
    "df_normal['label'] = 0                                  # 0 for normal tweets \n",
    "\n",
    "df_lonely = pd.read_csv('Data/Lonely_Tweets.csv', usecols=lambda column: column != 'Unnamed: 0')\n",
    "df_lonely.columns.values[0] = 'tweet'\n",
    "df_lonely['label'] = 1                                  # 1 for lonely tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a crudely filtered dataset. The quality of the uploaded data is low\n",
    "# and this creates large, fluctuating losses for the model training later. \n",
    "\n",
    "keywords = [\"alone\", \"lonely\", \"no one\"]\n",
    "filtered_df = df_lonely[df_lonely['tweet'].str.contains('|'.join(keywords), case=False, na=False)]\n",
    "df = pd.concat([df_normal, filtered_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data set has 11240 tweets in it.\n",
      "\n",
      "There are 2 variables and they are as follows: \n",
      "tweet    object\n",
      "label     int64\n",
      "dtype: object\n",
      "\n",
      "The first and last five rows of the table are:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>remember hillary email non secure server</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cant avoid demon</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>por fin la pusieron en spotify losing way de f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kills</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thank introduce important gunsense law make co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8504</th>\n",
       "      <td>love realize day dont want bother nothing wron...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8507</th>\n",
       "      <td>hi kerry believe alone please know thousand pe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8513</th>\n",
       "      <td>even though mean gotta let go yeah dependent y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8514</th>\n",
       "      <td>claire please message ever need talk lonely</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8519</th>\n",
       "      <td>lrt leave alone finally found need cry love ha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  label\n",
       "0             remember hillary email non secure server       0\n",
       "1                                     cant avoid demon       0\n",
       "2     por fin la pusieron en spotify losing way de f...      0\n",
       "3                                                kills       0\n",
       "4     thank introduce important gunsense law make co...      0\n",
       "8504  love realize day dont want bother nothing wron...      1\n",
       "8507  hi kerry believe alone please know thousand pe...      1\n",
       "8513  even though mean gotta let go yeah dependent y...      1\n",
       "8514       claire please message ever need talk lonely       1\n",
       "8519  lrt leave alone finally found need cry love ha...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display summary data\n",
    "print(f\"The data set has {len(df)} tweets in it.\")\n",
    "print()\n",
    "\n",
    "print(f\"There are {df.shape[1]} variables and they are as follows: \")\n",
    "print(df.dtypes)\n",
    "print()\n",
    "\n",
    "print(\"The first and last five rows of the table are:\")\n",
    "pd.concat([df.head(), df.tail()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data intoa a dataset file and spplit into \n",
    "# training and validation set\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.remove_columns(['__index_level_0__'])\n",
    "train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "test_valid_split = train_test_split['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "final_dataset = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'test': test_valid_split['test'],\n",
    "    'validation': test_valid_split['train']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer and model\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d70b89225cb4b52b363573a6d863ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8992 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee51e53220e41b4966c8ac4d83c93c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1124 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10b9d81864f4f7da01f601e533218dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1124 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['tweet'], truncation=True, padding='max_length', max_length=20)\n",
    "\n",
    "tokenized_dataset = final_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "os.makedirs('./logs', exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    log_level=\"error\"\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'], \n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473ed6e01d434e4c8bb1607abceec1b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3562, 'grad_norm': 1.5983589887619019, 'learning_rate': 4.955516014234875e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3894, 'grad_norm': 2.7363717555999756, 'learning_rate': 4.911032028469751e-05, 'epoch': 0.04}\n",
      "{'loss': 0.3611, 'grad_norm': 4.005592346191406, 'learning_rate': 4.8665480427046265e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2566, 'grad_norm': 6.693459987640381, 'learning_rate': 4.822064056939502e-05, 'epoch': 0.07}\n",
      "{'loss': 0.1117, 'grad_norm': 2.9570164680480957, 'learning_rate': 4.777580071174377e-05, 'epoch': 0.09}\n",
      "{'loss': 0.0795, 'grad_norm': 0.28621765971183777, 'learning_rate': 4.733096085409253e-05, 'epoch': 0.11}\n",
      "{'loss': 0.0449, 'grad_norm': 7.924477577209473, 'learning_rate': 4.6886120996441285e-05, 'epoch': 0.12}\n",
      "{'loss': 0.045, 'grad_norm': 0.09313103556632996, 'learning_rate': 4.644128113879004e-05, 'epoch': 0.14}\n",
      "{'loss': 0.0411, 'grad_norm': 0.09053602814674377, 'learning_rate': 4.599644128113879e-05, 'epoch': 0.16}\n",
      "{'loss': 0.1458, 'grad_norm': 0.2764173448085785, 'learning_rate': 4.555160142348754e-05, 'epoch': 0.18}\n",
      "{'loss': 0.1451, 'grad_norm': 1.7178272008895874, 'learning_rate': 4.5106761565836305e-05, 'epoch': 0.2}\n",
      "{'loss': 0.0681, 'grad_norm': 0.33052900433540344, 'learning_rate': 4.4661921708185054e-05, 'epoch': 0.21}\n",
      "{'loss': 0.0373, 'grad_norm': 0.17521415650844574, 'learning_rate': 4.421708185053381e-05, 'epoch': 0.23}\n",
      "{'loss': 0.1106, 'grad_norm': 0.30269068479537964, 'learning_rate': 4.377224199288256e-05, 'epoch': 0.25}\n",
      "{'loss': 0.1099, 'grad_norm': 2.7779343128204346, 'learning_rate': 4.3327402135231324e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0381, 'grad_norm': 0.10691925883293152, 'learning_rate': 4.2882562277580074e-05, 'epoch': 0.28}\n",
      "{'loss': 0.1427, 'grad_norm': 2.634538173675537, 'learning_rate': 4.2437722419928824e-05, 'epoch': 0.3}\n",
      "{'loss': 0.0345, 'grad_norm': 0.12496363371610641, 'learning_rate': 4.199288256227758e-05, 'epoch': 0.32}\n",
      "{'loss': 0.0028, 'grad_norm': 0.06597103923559189, 'learning_rate': 4.154804270462634e-05, 'epoch': 0.34}\n",
      "{'loss': 0.0825, 'grad_norm': 0.0879518911242485, 'learning_rate': 4.1103202846975093e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0034, 'grad_norm': 0.0821915790438652, 'learning_rate': 4.065836298932384e-05, 'epoch': 0.37}\n",
      "{'loss': 0.1081, 'grad_norm': 1.9815162420272827, 'learning_rate': 4.02135231316726e-05, 'epoch': 0.39}\n",
      "{'loss': 0.1054, 'grad_norm': 0.2650432288646698, 'learning_rate': 3.9768683274021356e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0626, 'grad_norm': 2.013838529586792, 'learning_rate': 3.932384341637011e-05, 'epoch': 0.43}\n",
      "{'loss': 0.0525, 'grad_norm': 0.12059376388788223, 'learning_rate': 3.887900355871886e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0726, 'grad_norm': 0.11833390593528748, 'learning_rate': 3.843416370106761e-05, 'epoch': 0.46}\n",
      "{'loss': 0.0895, 'grad_norm': 0.3141185939311981, 'learning_rate': 3.7989323843416376e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0618, 'grad_norm': 2.279813051223755, 'learning_rate': 3.7544483985765126e-05, 'epoch': 0.5}\n",
      "{'loss': 0.0325, 'grad_norm': 0.31695425510406494, 'learning_rate': 3.709964412811388e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0602, 'grad_norm': 2.425631046295166, 'learning_rate': 3.665480427046263e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0042, 'grad_norm': 0.06926361471414566, 'learning_rate': 3.6209964412811396e-05, 'epoch': 0.55}\n",
      "{'loss': 0.1293, 'grad_norm': 0.08589892089366913, 'learning_rate': 3.5765124555160145e-05, 'epoch': 0.57}\n",
      "{'loss': 0.2059, 'grad_norm': 0.4888981878757477, 'learning_rate': 3.53202846975089e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0705, 'grad_norm': 6.593468189239502, 'learning_rate': 3.487544483985765e-05, 'epoch': 0.6}\n",
      "{'loss': 0.1333, 'grad_norm': 0.06732366979122162, 'learning_rate': 3.44306049822064e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0433, 'grad_norm': 0.09224486351013184, 'learning_rate': 3.3985765124555165e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0808, 'grad_norm': 0.08372795581817627, 'learning_rate': 3.3540925266903915e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0715, 'grad_norm': 0.330458402633667, 'learning_rate': 3.309608540925267e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0897, 'grad_norm': 0.20223373174667358, 'learning_rate': 3.265124555160142e-05, 'epoch': 0.69}\n",
      "{'loss': 0.0242, 'grad_norm': 0.070701964199543, 'learning_rate': 3.2206405693950184e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0384, 'grad_norm': 0.06624855846166611, 'learning_rate': 3.1761565836298934e-05, 'epoch': 0.73}\n",
      "{'loss': 0.0594, 'grad_norm': 0.04295889288187027, 'learning_rate': 3.1316725978647684e-05, 'epoch': 0.75}\n",
      "{'loss': 0.0633, 'grad_norm': 3.620906114578247, 'learning_rate': 3.087188612099644e-05, 'epoch': 0.77}\n",
      "{'loss': 0.094, 'grad_norm': 0.3424074947834015, 'learning_rate': 3.04270462633452e-05, 'epoch': 0.78}\n",
      "{'loss': 0.1203, 'grad_norm': 0.05287737026810646, 'learning_rate': 2.9982206405693954e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0603, 'grad_norm': 0.07930832356214523, 'learning_rate': 2.9537366548042704e-05, 'epoch': 0.82}\n",
      "{'loss': 0.0025, 'grad_norm': 0.035341355949640274, 'learning_rate': 2.9092526690391457e-05, 'epoch': 0.84}\n",
      "{'loss': 0.0015, 'grad_norm': 0.05610461160540581, 'learning_rate': 2.8647686832740217e-05, 'epoch': 0.85}\n",
      "{'loss': 0.0612, 'grad_norm': 0.04026133194565773, 'learning_rate': 2.820284697508897e-05, 'epoch': 0.87}\n",
      "{'loss': 0.0581, 'grad_norm': 0.09320840984582901, 'learning_rate': 2.7758007117437723e-05, 'epoch': 0.89}\n",
      "{'loss': 0.0372, 'grad_norm': 0.028508968651294708, 'learning_rate': 2.7313167259786476e-05, 'epoch': 0.91}\n",
      "{'loss': 0.0456, 'grad_norm': 0.07226008921861649, 'learning_rate': 2.6868327402135236e-05, 'epoch': 0.93}\n",
      "{'loss': 0.0532, 'grad_norm': 0.10421142727136612, 'learning_rate': 2.642348754448399e-05, 'epoch': 0.94}\n",
      "{'loss': 0.0364, 'grad_norm': 0.06016891077160835, 'learning_rate': 2.597864768683274e-05, 'epoch': 0.96}\n",
      "{'loss': 0.0583, 'grad_norm': 0.10281588137149811, 'learning_rate': 2.5533807829181493e-05, 'epoch': 0.98}\n",
      "{'loss': 0.0122, 'grad_norm': 0.10473665595054626, 'learning_rate': 2.5088967971530253e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a9498b37a8410c9a9ac648e1281256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/141 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.032578952610492706, 'eval_runtime': 8.3741, 'eval_samples_per_second': 134.224, 'eval_steps_per_second': 16.838, 'epoch': 1.0}\n",
      "{'loss': 0.0368, 'grad_norm': 0.04511674866080284, 'learning_rate': 2.4644128113879006e-05, 'epoch': 1.01}\n",
      "{'loss': 0.0817, 'grad_norm': 0.024781858548521996, 'learning_rate': 2.419928825622776e-05, 'epoch': 1.03}\n",
      "{'loss': 0.0022, 'grad_norm': 0.028053469955921173, 'learning_rate': 2.3754448398576516e-05, 'epoch': 1.05}\n",
      "{'loss': 0.0015, 'grad_norm': 0.04837877303361893, 'learning_rate': 2.330960854092527e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0335, 'grad_norm': 0.04722357913851738, 'learning_rate': 2.2864768683274025e-05, 'epoch': 1.09}\n",
      "{'loss': 0.001, 'grad_norm': 0.0251116082072258, 'learning_rate': 2.2419928825622775e-05, 'epoch': 1.1}\n",
      "{'loss': 0.0307, 'grad_norm': 0.01736677810549736, 'learning_rate': 2.197508896797153e-05, 'epoch': 1.12}\n",
      "{'loss': 0.0013, 'grad_norm': 0.0642259418964386, 'learning_rate': 2.1530249110320285e-05, 'epoch': 1.14}\n",
      "{'loss': 0.0016, 'grad_norm': 0.021849120035767555, 'learning_rate': 2.1085409252669038e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0533, 'grad_norm': 0.0381070040166378, 'learning_rate': 2.0640569395017795e-05, 'epoch': 1.17}\n",
      "{'loss': 0.0074, 'grad_norm': 0.019457576796412468, 'learning_rate': 2.0195729537366548e-05, 'epoch': 1.19}\n",
      "{'loss': 0.0286, 'grad_norm': 0.03020213544368744, 'learning_rate': 1.9750889679715305e-05, 'epoch': 1.21}\n",
      "{'loss': 0.0518, 'grad_norm': 0.028025303035974503, 'learning_rate': 1.9306049822064058e-05, 'epoch': 1.23}\n",
      "{'loss': 0.0418, 'grad_norm': 7.501309394836426, 'learning_rate': 1.8861209964412814e-05, 'epoch': 1.25}\n",
      "{'loss': 0.1031, 'grad_norm': 5.479750156402588, 'learning_rate': 1.8416370106761564e-05, 'epoch': 1.26}\n",
      "{'loss': 0.0672, 'grad_norm': 0.07691486179828644, 'learning_rate': 1.797153024911032e-05, 'epoch': 1.28}\n",
      "{'loss': 0.0334, 'grad_norm': 0.5570674538612366, 'learning_rate': 1.7526690391459074e-05, 'epoch': 1.3}\n",
      "{'loss': 0.0299, 'grad_norm': 0.023686906322836876, 'learning_rate': 1.708185053380783e-05, 'epoch': 1.32}\n",
      "{'loss': 0.0647, 'grad_norm': 0.05250975489616394, 'learning_rate': 1.6637010676156584e-05, 'epoch': 1.33}\n",
      "{'loss': 0.0071, 'grad_norm': 0.027620522305369377, 'learning_rate': 1.619217081850534e-05, 'epoch': 1.35}\n",
      "{'loss': 0.0325, 'grad_norm': 0.057200849056243896, 'learning_rate': 1.5747330960854093e-05, 'epoch': 1.37}\n",
      "{'loss': 0.0306, 'grad_norm': 0.04588798061013222, 'learning_rate': 1.530249110320285e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0475, 'grad_norm': 0.011003789491951466, 'learning_rate': 1.4857651245551602e-05, 'epoch': 1.41}\n",
      "{'loss': 0.0015, 'grad_norm': 0.06555014103651047, 'learning_rate': 1.4412811387900358e-05, 'epoch': 1.42}\n",
      "{'loss': 0.1054, 'grad_norm': 0.06607300043106079, 'learning_rate': 1.396797153024911e-05, 'epoch': 1.44}\n",
      "{'loss': 0.0779, 'grad_norm': 3.585934638977051, 'learning_rate': 1.3523131672597866e-05, 'epoch': 1.46}\n",
      "{'loss': 0.0359, 'grad_norm': 0.04210789501667023, 'learning_rate': 1.307829181494662e-05, 'epoch': 1.48}\n",
      "{'loss': 0.0026, 'grad_norm': 0.1383245438337326, 'learning_rate': 1.2633451957295376e-05, 'epoch': 1.49}\n",
      "{'loss': 0.0028, 'grad_norm': 0.13109345734119415, 'learning_rate': 1.2188612099644127e-05, 'epoch': 1.51}\n",
      "{'loss': 0.0324, 'grad_norm': 0.1232748031616211, 'learning_rate': 1.1743772241992882e-05, 'epoch': 1.53}\n",
      "{'loss': 0.0083, 'grad_norm': 0.10024362057447433, 'learning_rate': 1.1298932384341637e-05, 'epoch': 1.55}\n",
      "{'loss': 0.0775, 'grad_norm': 0.05010353773832321, 'learning_rate': 1.0854092526690392e-05, 'epoch': 1.57}\n",
      "{'loss': 0.0419, 'grad_norm': 0.12159587442874908, 'learning_rate': 1.0409252669039145e-05, 'epoch': 1.58}\n",
      "{'loss': 0.0031, 'grad_norm': 0.0720549076795578, 'learning_rate': 9.9644128113879e-06, 'epoch': 1.6}\n",
      "{'loss': 0.0023, 'grad_norm': 0.007491809315979481, 'learning_rate': 9.519572953736655e-06, 'epoch': 1.62}\n",
      "{'loss': 0.0031, 'grad_norm': 0.08567454665899277, 'learning_rate': 9.07473309608541e-06, 'epoch': 1.64}\n",
      "{'loss': 0.0018, 'grad_norm': 0.05633043870329857, 'learning_rate': 8.629893238434163e-06, 'epoch': 1.65}\n",
      "{'loss': 0.0318, 'grad_norm': 0.025405865162611008, 'learning_rate': 8.185053380782918e-06, 'epoch': 1.67}\n",
      "{'loss': 0.0631, 'grad_norm': 0.0586879663169384, 'learning_rate': 7.740213523131673e-06, 'epoch': 1.69}\n",
      "{'loss': 0.0103, 'grad_norm': 0.01937125064432621, 'learning_rate': 7.295373665480428e-06, 'epoch': 1.71}\n",
      "{'loss': 0.0013, 'grad_norm': 0.03247906267642975, 'learning_rate': 6.850533807829182e-06, 'epoch': 1.73}\n",
      "{'loss': 0.0011, 'grad_norm': 0.03525731340050697, 'learning_rate': 6.405693950177937e-06, 'epoch': 1.74}\n",
      "{'loss': 0.0029, 'grad_norm': 0.005446600262075663, 'learning_rate': 5.960854092526691e-06, 'epoch': 1.76}\n",
      "{'loss': 0.0011, 'grad_norm': 0.05260494351387024, 'learning_rate': 5.516014234875446e-06, 'epoch': 1.78}\n",
      "{'loss': 0.0272, 'grad_norm': 0.027746656909585, 'learning_rate': 5.0711743772242e-06, 'epoch': 1.8}\n",
      "{'loss': 0.0008, 'grad_norm': 0.031799282878637314, 'learning_rate': 4.626334519572954e-06, 'epoch': 1.81}\n",
      "{'loss': 0.0009, 'grad_norm': 0.013893903233110905, 'learning_rate': 4.181494661921708e-06, 'epoch': 1.83}\n",
      "{'loss': 0.0009, 'grad_norm': 0.081949383020401, 'learning_rate': 3.7366548042704624e-06, 'epoch': 1.85}\n",
      "{'loss': 0.0268, 'grad_norm': 0.02419251948595047, 'learning_rate': 3.291814946619217e-06, 'epoch': 1.87}\n",
      "{'loss': 0.0235, 'grad_norm': 0.025511134415864944, 'learning_rate': 2.8469750889679713e-06, 'epoch': 1.89}\n",
      "{'loss': 0.0011, 'grad_norm': 0.039971571415662766, 'learning_rate': 2.402135231316726e-06, 'epoch': 1.9}\n",
      "{'loss': 0.0169, 'grad_norm': 73.56675720214844, 'learning_rate': 1.9572953736654803e-06, 'epoch': 1.92}\n",
      "{'loss': 0.0358, 'grad_norm': 0.02651144191622734, 'learning_rate': 1.512455516014235e-06, 'epoch': 1.94}\n",
      "{'loss': 0.0181, 'grad_norm': 0.015895288437604904, 'learning_rate': 1.0676156583629894e-06, 'epoch': 1.96}\n",
      "{'loss': 0.0009, 'grad_norm': 0.023898309096693993, 'learning_rate': 6.227758007117438e-07, 'epoch': 1.98}\n",
      "{'loss': 0.001, 'grad_norm': 0.038652632385492325, 'learning_rate': 1.779359430604982e-07, 'epoch': 1.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49f93641f4043289200656fea3e7b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/141 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.030920151621103287, 'eval_runtime': 8.9155, 'eval_samples_per_second': 126.072, 'eval_steps_per_second': 15.815, 'epoch': 2.0}\n",
      "{'train_runtime': 501.1179, 'train_samples_per_second': 35.888, 'train_steps_per_second': 2.243, 'train_loss': 0.056678998426557436, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1124, training_loss=0.056678998426557436, metrics={'train_runtime': 501.1179, 'train_samples_per_second': 35.888, 'train_steps_per_second': 2.243, 'total_flos': 184835516390400.0, 'train_loss': 0.056678998426557436, 'epoch': 2.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert_model/tokenizer_config.json',\n",
       " 'bert_model/special_tokens_map.json',\n",
       " 'bert_model/vocab.txt',\n",
       " 'bert_model/added_tokens.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"bert_model\")\n",
    "tokenizer.save_pretrained(\"bert_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model classified the tweet as: lonely\n"
     ]
    }
   ],
   "source": [
    "# Classify an example tweet\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"./bert_model\", tokenizer=\"./bert_model\")\n",
    "tweet = \"bananarama is lonely\"\n",
    "result = classifier(tweet)\n",
    "label = result[0]['label']\n",
    "print(f\"The model classified the tweet as: {'not lonely' if label == 'LABEL_0' else 'lonely'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
